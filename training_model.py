# -*- coding: utf-8 -*-
"""training_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_AeFgJbt6-bNipx7GZgBAq57x5SL5MjH
"""

!pip install datasets
!pip install xgboost
!pip install scikit-learn
!pip install pandas numpy

import pandas as pd
import numpy as np
import re
import math
from urllib.parse import urlparse
from collections import Counter
from datasets import load_dataset

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import xgboost as xgb
import joblib

print("‚úÖ All libraries imported successfully!")

def extract_features(url):
    """
    Extract handcrafted features from a URL for phishing detection.
    Compatible with both Colab training and Streamlit deployment.
    """
    features = {}

    try:
        # Parse the URL
        parsed = urlparse(url)
        hostname = parsed.netloc
        path = parsed.path
        query = parsed.query

        # 1. URL Length Features
        features['url_length'] = len(url)
        features['hostname_length'] = len(hostname)
        features['path_length'] = len(path)

        # 2. Count Special Characters
        features['num_dots'] = url.count('.')
        features['num_hyphens'] = url.count('-')
        features['num_underscores'] = url.count('_')
        features['num_slashes'] = url.count('/')
        features['num_question_marks'] = url.count('?')
        features['num_equals'] = url.count('=')
        features['num_at'] = url.count('@')
        features['num_ampersands'] = url.count('&')
        features['num_percent'] = url.count('%')

        # 3. Count Digits
        features['num_digits'] = sum(c.isdigit() for c in url)
        features['digit_ratio'] = sum(c.isdigit() for c in url) / max(len(url), 1)

        # 4. Special Character Ratio
        special_chars = len(re.findall(r'[^a-zA-Z0-9]', url))
        features['special_char_ratio'] = special_chars / max(len(url), 1)

        # 5. Number of Subdomains
        if hostname:
            subdomains = hostname.split('.')
            features['num_subdomains'] = max(len(subdomains) - 2, 0)
        else:
            features['num_subdomains'] = 0

        # 6. HTTPS Check
        features['is_https'] = 1 if parsed.scheme == 'https' else 0

        # 7. IP Address Detection
        ip_pattern = r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'
        features['has_ip_address'] = 1 if re.search(ip_pattern, hostname) else 0

        # 8. Suspicious TLD Check
        suspicious_tlds = ['.tk', '.ml', '.ga', '.cf', '.gq', '.xyz', '.top', '.work', '.click']
        features['suspicious_tld'] = 1 if any(url.endswith(tld) for tld in suspicious_tlds) else 0

        # 9. Query Parameters Count
        features['num_query_params'] = len(query.split('&')) if query else 0

        # 10. Shannon Entropy of Hostname
        if hostname:
            entropy = 0
            for count in Counter(hostname).values():
                probability = count / len(hostname)
                entropy -= probability * math.log2(probability)
            features['hostname_entropy'] = entropy
        else:
            features['hostname_entropy'] = 0

        # 11. Suspicious Keywords
        suspicious_keywords = ['login', 'signin', 'bank', 'account', 'update',
                              'verify', 'secure', 'password', 'confirm', 'admin']
        features['has_suspicious_keyword'] = 1 if any(kw in url.lower() for kw in suspicious_keywords) else 0

        # 12. Abnormal URL Features
        features['abnormal_url'] = 1 if hostname and hostname not in url else 0

        # 13. Double Slash in Path
        features['double_slash_redirecting'] = 1 if '//' in path else 0

    except Exception as e:
        print(f"Error parsing URL: {url}, Error: {e}")
        features = {
            'url_length': 0, 'hostname_length': 0, 'path_length': 0,
            'num_dots': 0, 'num_hyphens': 0, 'num_underscores': 0,
            'num_slashes': 0, 'num_question_marks': 0, 'num_equals': 0,
            'num_at': 0, 'num_ampersands': 0, 'num_percent': 0,
            'num_digits': 0, 'digit_ratio': 0, 'special_char_ratio': 0,
            'num_subdomains': 0, 'is_https': 0, 'has_ip_address': 0,
            'suspicious_tld': 0, 'num_query_params': 0, 'hostname_entropy': 0,
            'has_suspicious_keyword': 0, 'abnormal_url': 0, 'double_slash_redirecting': 0
        }

    return features

print("‚úÖ Feature extraction function defined!")

print("Loading dataset from HuggingFace...")
dataset = load_dataset("shawhin/phishing-site-classification")

# Convert to pandas DataFrame
df = pd.DataFrame(dataset['train'])

print(f"‚úÖ Dataset loaded successfully!")
print(f"Total samples: {len(df)}")
print(f"\nDataset columns: {df.columns.tolist()}")
print(f"\nFirst few rows:")
print(df.head())

# Show label distribution (handle different column names)
label_col = 'label' if 'label' in df.columns else 'labels'
print(f"\nLabel distribution:")
print(df[label_col].value_counts())

print("Cleaning and renaming columns...")

# Rename columns to standard names
if 'text' in df.columns:
    df = df.rename(columns={'text': 'url'})
elif 'url' not in df.columns:
    # Find the URL column
    url_col = [col for col in df.columns if 'url' in col.lower() or 'text' in col.lower()][0]
    df = df.rename(columns={url_col: 'url'})

if 'labels' in df.columns:
    df = df.rename(columns={'labels': 'label'})
elif 'label' not in df.columns:
    # Find the label column
    label_col = [col for col in df.columns if 'label' in col.lower() or 'target' in col.lower()][0]
    df = df.rename(columns={label_col: 'label'})

# Remove any rows with missing URLs
df = df.dropna(subset=['url'])

# Ensure labels are binary (0 and 1)
# Handle various label formats
if df['label'].dtype == 'object' or df['label'].dtype == 'string':
    df['label'] = df['label'].map({
        'legitimate': 0, 'phishing': 1,
        'good': 0, 'bad': 1,
        'benign': 0, 'malicious': 1,
        '0': 0, '1': 1,
        0: 0, 1: 1
    })
else:
    # Already numeric, ensure it's 0 or 1
    df['label'] = df['label'].astype(int)

# Remove any remaining NaN labels
df = df.dropna(subset=['label'])

# Verify labels are only 0 and 1
unique_labels = df['label'].unique()
print(f"Unique labels found: {unique_labels}")

print(f"\n‚úÖ Data cleaned!")
print(f"Final dataset size: {len(df)}")
print(f"Columns: {df.columns.tolist()}")
print(f"\nLabel distribution:")
print(f"  Legitimate (0): {sum(df['label'] == 0)}")
print(f"  Phishing (1): {sum(df['label'] == 1)}")

print("Extracting features from all URLs...")
print("This may take a few minutes...")

features_list = []
for idx, url in enumerate(df['url']):
    if idx % 1000 == 0:
        print(f"Progress: {idx}/{len(df)} URLs processed", end='\r')
    features_list.append(extract_features(url))

print(f"\nProgress: {len(df)}/{len(df)} URLs processed - Complete!")

# Create features DataFrame
X = pd.DataFrame(features_list)
y = df['label'].values

print(f"\n‚úÖ Feature extraction complete!")
print(f"Feature matrix shape: {X.shape}")
print(f"Features: {X.columns.tolist()}")
print(f"\nFeature statistics:")
print(X.describe())

print("\nSplitting data into train and test sets (80-20)...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"‚úÖ Data split complete!")
print(f"Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)")
print(f"Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)")
print(f"Training - Phishing: {sum(y_train)}, Legitimate: {len(y_train) - sum(y_train)}")
print(f"Testing - Phishing: {sum(y_test)}, Legitimate: {len(y_test) - sum(y_test)}")

print("\n" + "="*60)
print("TRAINING RANDOM FOREST MODEL")
print("="*60)

rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=20,
    random_state=42,
    n_jobs=-1,
    verbose=1
)

print("Training Random Forest...")
rf_model.fit(X_train, y_train)
print("‚úÖ Random Forest training complete!")

# Make predictions
y_pred_rf = rf_model.predict(X_test)

# Calculate metrics
rf_accuracy = accuracy_score(y_test, y_pred_rf)

print("\n" + "="*60)
print("RANDOM FOREST - MODEL EVALUATION")
print("="*60)
print(f"\nüéØ Accuracy: {rf_accuracy * 100:.2f}%\n")
print("Classification Report:")
print(classification_report(y_test, y_pred_rf, target_names=['Legitimate', 'Phishing']))
print("\nConfusion Matrix:")
cm_rf = confusion_matrix(y_test, y_pred_rf)
print(f"                 Predicted")
print(f"              Legit  Phishing")
print(f"Actual Legit   {cm_rf[0][0]:5d}    {cm_rf[0][1]:5d}")
print(f"     Phishing  {cm_rf[1][0]:5d}    {cm_rf[1][1]:5d}")

# Feature Importance
print("\nüìä Top 10 Most Important Features (Random Forest):")
feature_importance_rf = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

for idx, row in feature_importance_rf.head(10).iterrows():
    print(f"  {row['feature']:30s} {row['importance']:.4f}")

print("\n" + "="*60)
print("TRAINING XGBOOST MODEL")
print("="*60)

xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=10,
    learning_rate=0.1,
    random_state=42,
    n_jobs=-1,
    verbosity=1
)

print("Training XGBoost...")
xgb_model.fit(X_train, y_train)
print("‚úÖ XGBoost training complete!")

# Make predictions
y_pred_xgb = xgb_model.predict(X_test)

# Calculate metrics
xgb_accuracy = accuracy_score(y_test, y_pred_xgb)

print("\n" + "="*60)
print("XGBOOST - MODEL EVALUATION")
print("="*60)
print(f"\nüéØ Accuracy: {xgb_accuracy * 100:.2f}%\n")
print("Classification Report:")
print(classification_report(y_test, y_pred_xgb, target_names=['Legitimate', 'Phishing']))
print("\nConfusion Matrix:")
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
print(f"                 Predicted")
print(f"              Legit  Phishing")
print(f"Actual Legit   {cm_xgb[0][0]:5d}    {cm_xgb[0][1]:5d}")
print(f"     Phishing  {cm_xgb[1][0]:5d}    {cm_xgb[1][1]:5d}")

# Feature Importance
print("\nüìä Top 10 Most Important Features (XGBoost):")
feature_importance_xgb = pd.DataFrame({
    'feature': X.columns,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False)

for idx, row in feature_importance_xgb.head(10).iterrows():
    print(f"  {row['feature']:30s} {row['importance']:.4f}")

print("\n" + "="*60)
print("MODEL COMPARISON")
print("="*60)
print(f"Random Forest Accuracy: {rf_accuracy * 100:.2f}%")
print(f"XGBoost Accuracy:       {xgb_accuracy * 100:.2f}%")

if rf_accuracy >= xgb_accuracy:
    best_model = rf_model
    best_model_name = "Random Forest"
    best_accuracy = rf_accuracy
    print(f"\nüèÜ Best Model: Random Forest")
else:
    best_model = xgb_model
    best_model_name = "XGBoost"
    best_accuracy = xgb_accuracy
    print(f"\nüèÜ Best Model: XGBoost")

print(f"Best Model Accuracy: {best_accuracy * 100:.2f}%")

print("\n" + "="*60)
print("SAVING MODEL AND FEATURES")
print("="*60)

# Save the best model
joblib.dump(best_model, 'phishing_detector_model.pkl')
print(f"‚úÖ Model saved as 'phishing_detector_model.pkl'")

# Save feature column names (important for consistent prediction)
feature_columns = X.columns.tolist()
joblib.dump(feature_columns, 'feature_columns.pkl')
print(f"‚úÖ Feature columns saved as 'feature_columns.pkl'")

# Save model info
model_info = {
    'model_name': best_model_name,
    'accuracy': best_accuracy,
    'n_features': len(feature_columns),
    'feature_names': feature_columns
}
joblib.dump(model_info, 'model_info.pkl')
print(f"‚úÖ Model info saved as 'model_info.pkl'")

print("\n" + "="*60)
print("‚úÖ TRAINING COMPLETE!")
print("="*60)
print("\nFiles created:")
print("  1. phishing_detector_model.pkl")
print("  2. feature_columns.pkl")
print("  3. model_info.pkl")
print("\nDownload these files to use with Streamlit app!")
print("="*60)

from google.colab import files
files.download('phishing_detector_model.pkl')
files.download('feature_columns.pkl')
files.download('model_info.pkl')
